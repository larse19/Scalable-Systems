version: "3.9"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.1
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node1"
    ports:
      - 2181:2181
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.2.1
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node1"
    ports:
      - 9092:9092
      - 9094:9094
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER_LISTENER:PLAINTEXT,OUTSIDE:PLAINTEXT"
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9091,EXTERNAL://0.0.0.0:19092,DOCKER_LISTENER://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka:9091,EXTERNAL://host.docker.internal:19092,DOCKER_LISTENER://kafka:9092,OUTSIDE://localhost:9094"
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_PARTITION_ASSIGNMENT_STRATEGY: org.apache.kafka.clients.consumer.RoundRobinAssignor
    restart: unless-stopped
    depends_on:
      - zookeeper

  kowl:
    image: quay.io/cloudhut/kowl:master # We use the master tag as we want to use the latest features e.g. creation of topics.
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node1"
    ports:
      - 8888:8080
    restart: unless-stopped
    depends_on:
      - zookeeper
      - kafka
    environment:
      KAFKA_BROKERS: kafka:9092

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node1"
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - INIT_DAEMON_STEP=setup_spark

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node2"
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node2"
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - 8082:8082
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  cassandra1:
    image: cassandra:4.0
    container_name: cassandra1
    deploy:
      placement:
        constraints:
          - "node.hostname==bddst-g13-Node3"
    ports:
      - 7000:7000
      - 9042:9042
    volumes:
      - cassandra_data:/var/lib/cassandra
    environment:
      - CASSANDRA_CLUSTER_NAME=cass-cluster
    restart: always

volumes:
  cassandra_data:

networks:
  default:
    name: big-data-network
    attachable: true